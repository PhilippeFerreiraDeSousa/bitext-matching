{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import IBMModel1\n",
    "import IBMModel2\n",
    "import string\n",
    "import numpy as np\n",
    "import Tools\n",
    "\n",
    "\n",
    "def create_tokenized_sentences(content_list, max_index):\n",
    "    return_sentence_list = list()\n",
    "    word_dictionary = {} #this dictionary will keep both word and its order in its language\n",
    "    lang_order = 0\n",
    "    cnt = 0\n",
    "    max_len_sentence = 0\n",
    "    translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "    for row in content_list[:max_index]:\n",
    "        if cnt == 0 :\n",
    "            row = row.replace(u'\\ufeff', '')\n",
    "            cnt += 1\n",
    "\n",
    "        row = row.translate(translate_table)\n",
    "        tokens = word_tokenize(row.lower())\n",
    "\n",
    "        if len(tokens) > max_len_sentence :\n",
    "           max_len_sentence = len(tokens)\n",
    "\n",
    "        produced_sentence = \"\"\n",
    "        for token in tokens:\n",
    "            if token not in word_dictionary:\n",
    "                word_dictionary[token] = lang_order\n",
    "                lang_order += 1\n",
    "            produced_sentence = produced_sentence + token + \" \"\n",
    "        produced_sentence = produced_sentence[:(len(produced_sentence) - 1)]  # remove last empty\n",
    "\n",
    "        return_sentence_list.append(produced_sentence)\n",
    "\n",
    "    return_sentence_list[0] = return_sentence_list[0].replace(u'\\ufeff', '')  # ufeff character from document start\n",
    "    return return_sentence_list, word_dictionary,max_len_sentence\n",
    "\n",
    "\n",
    "def train_models():\n",
    "    with open(\"Dictionary_files\\PP_en.txt\", encoding=\"utf8\") as f:\n",
    "        content_en = f.readlines()\n",
    "\n",
    "    with open(\"Dictionary_files\\PP_fr.txt\", encoding=\"utf8\") as f:\n",
    "        content_fr = f.readlines()\n",
    "\n",
    "    #sentences with length at most 10 words.\n",
    "    new_content_en = list()\n",
    "    new_content_fr = list()\n",
    "\n",
    "    for sen_idx in range(len(content_en)):\n",
    "        cur_en_sen = content_en[sen_idx].split()\n",
    "        cur_fr_sen = content_fr[sen_idx].split()\n",
    "        if len(cur_en_sen) < 11 and len(cur_fr_sen) < 11:\n",
    "            new_content_en.append(content_en[sen_idx])\n",
    "            new_content_fr.append(content_fr[sen_idx])\n",
    "\n",
    "    content_en = new_content_en.copy()\n",
    "    content_fr = new_content_fr.copy()\n",
    "\n",
    "\n",
    "    max_num_of_franslations = Common.num_of_frain_sample\n",
    "\n",
    "    # parse foreign sentences, tokenize ALL the words :D !\n",
    "    foreign_sentences, foreign_word_dict, max_le = create_tokenized_sentences(content_fr, max_num_of_franslations)\n",
    "\n",
    "    # parse english sentences, tokenize ALL the words :D !\n",
    "    english_sentences, english_word_dict, max_lf = create_tokenized_sentences(content_en, max_num_of_franslations)\n",
    "\n",
    "    print(content_en[145:150])\n",
    "    print(content_fr[145:150])\n",
    "\n",
    "    print(english_sentences[145:150])\n",
    "    print(foreign_sentences[145:150])\n",
    "\n",
    "    print(max_le)\n",
    "    print(max_lf)\n",
    "\n",
    "    '''\n",
    "    np.save(\"models/e_word_dict\",foreign_word_dict)\n",
    "    np.save(\"models/f_word_dict\",english_word_dict)\n",
    "    t_e_f  = IBMModel1.expectation_maximization(foreign_word_dict,english_word_dict,foreign_sentences,english_sentences)\n",
    "    np.save(\"models/t_e_f_mat_model1\",t_e_f)\n",
    "    t_e_f, a_i_le_lf_mat = IBMModel2.train(t_e_f,foreign_word_dict,english_word_dict,foreign_sentences,english_sentences,max_le,max_lf)\n",
    "    np.save(\"models/t_e_f_mat_model2\",t_e_f)\n",
    "    np.save(\"models/a_i_le_lf_mat_model2\",a_i_le_lf_mat)\n",
    "    '''\n",
    "\n",
    "    t_e_f = np.load('models/t_e_f_mat_model2.npy')\n",
    "    a_i_le_lf_mat = np.load('models/a_i_le_lf_mat_model2.npy')\n",
    "   \n",
    "    np.save(\"models/p0\",p0)\n",
    "    np.save(\"models/p1\",p1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
